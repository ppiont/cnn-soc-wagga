{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "<a href=\"https://colab.research.google.com/github/ppiont/cnn-soc-wagga/blob/master/mlp_colab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ],
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "pip install scikit-optimize"
      ],
      "outputs": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "blbFec8MEbhF",
        "outputId": "7e8d5f31-adaf-461c-bb1c-a569079a03f0"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')\n",
        "%cd \"/content/drive/MyDrive/Thesis/cnn-soc-wagga\""
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive/\n",
            "/content/drive/MyDrive/Thesis/cnn-soc-wagga\n"
          ]
        }
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6OEVoc-YEoEu",
        "outputId": "5d319b83-7cf1-4f7d-98ac-d98b1ed40073"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "source": [
        "# Standard lib imports\n",
        "import os\n",
        "import pathlib\n",
        "import random\n",
        "\n",
        "# Imports\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from skopt import gp_minimize, dump, load\n",
        "from skopt.space import Integer, Real, Categorical\n",
        "from skopt.utils import use_named_args\n",
        "from skopt.plots import plot_objective, plot_convergence, plot_evaluations\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader\n",
        "import copy\n",
        "import pdb  # Brug det"
      ],
      "outputs": [],
      "metadata": {
        "id": "LisxU8b2EMcJ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "source": [
        "# ------------------- TO DO ------------------------------------------------- #\n",
        "\n",
        "\"\"\"\n",
        "Use Torch Dataset.. you made a class for it dummy\n",
        "\"\"\"\n",
        "\n",
        "# ------------------- Settings ---------------------------------------------- #\n",
        "\n",
        "\n",
        "# Set matploblib style\n",
        "plt.style.use('seaborn-colorblind')\n",
        "colors = plt.rcParams['axes.prop_cycle'].by_key()['color']\n",
        "plt.rcParams['figure.dpi'] = 450\n",
        "plt.rcParams['savefig.transparent'] = True\n",
        "plt.rcParams['savefig.format'] = 'svg'\n",
        "\n",
        "# Reset params if needed\n",
        "# plt.rcParams.update(mpl.rcParamsDefault)\n",
        "\n",
        "\n",
        "# ------------------- Organization ------------------------------------------ #\n",
        "\n",
        "\n",
        "DATA_DIR = pathlib.Path('data/')\n",
        "\n",
        "\n",
        "def seed_everything(SEED=43):\n",
        "    random.seed(SEED)\n",
        "    np.random.seed(SEED)\n",
        "    torch.manual_seed(SEED)\n",
        "    torch.cuda.manual_seed(SEED)\n",
        "    torch.cuda.manual_seed_all(SEED)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    os.environ['PYTHONHASHSEED'] = str(SEED)\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "\n",
        "\n",
        "SEED = 43\n",
        "seed_everything(SEED=SEED)\n",
        "\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "torch.set_default_tensor_type('torch.cuda.FloatTensor')"
      ],
      "outputs": [],
      "metadata": {
        "id": "cbG11p-dGEVA"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "source": [
        "# ------------------- Read and prep data ------------------------------------ #\n",
        "\n",
        "train_data = np.load(DATA_DIR.joinpath('train_45.npy'))\n",
        "test_data = np.load(DATA_DIR.joinpath('test_45.npy'))\n",
        "\n",
        "x_train = train_data[:, 3:]\n",
        "y_train = train_data[:, 0]\n",
        "\n",
        "input_dims = x_train.shape[-1]\n",
        "\n",
        "x_test = test_data[:, 3:]\n",
        "y_test = test_data[:, 0]\n",
        "\n",
        "# Normalize X\n",
        "scaler_x = MinMaxScaler()\n",
        "scaler_x.fit(x_train)\n",
        "x_train = scaler_x.transform(x_train)\n",
        "x_test = scaler_x.transform(x_test)\n",
        "\n",
        "# Normalize y\n",
        "scaler_y = MinMaxScaler()\n",
        "scaler_y.fit(y_train.reshape(-1, 1))\n",
        "y_train = scaler_y.fit_transform(y_train.reshape(-1, 1))\n",
        "# There is no reason to scale y_test actually\n",
        "y_test = scaler_y.transform(y_test.reshape(-1, 1))\n",
        "\n",
        "\n",
        "x_train, y_train = torch.from_numpy(x_train), torch.from_numpy(y_train)\n",
        "x_test, y_test = torch.from_numpy(x_test), torch.from_numpy(y_test)\n",
        "\n",
        "x_train, x_test = x_train.to(device), x_test.to(device)\n",
        "y_train, y_test = y_train.to(device), y_test.to(device)\n",
        "\n",
        "class Dataset(torch.utils.data.TensorDataset):\n",
        "    \"\"\"Characterize a PyTorch Dataset.\"\"\"\n",
        "\n",
        "    def __init__(self, features, targets):\n",
        "        \"\"\"Initialize with X and y.\"\"\"\n",
        "        self.features = features\n",
        "        self.targets = targets\n",
        "\n",
        "    def __len__(self):\n",
        "        \"\"\"Return total number of samples.\"\"\"\n",
        "        return len(self.targets)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        \"\"\"Generate one data sample.\"\"\"\n",
        "        return self.features[index], self.targets[index]"
      ],
      "outputs": [],
      "metadata": {
        "id": "FjTB4-GdGEZD"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "source": [
        "# ------------------- NN setup ---------------------------------------------- #\n",
        "\n",
        "class NeuralNet(nn.Module):\n",
        "    \"\"\"Neural Network class.\"\"\"\n",
        "\n",
        "    def __init__(self, input_dims=input_dims, n_layers=3, n_neurons=64,\n",
        "                 activation=nn.ELU(), dropout_rate=0.5):\n",
        "        \"\"\"Initialize as subclass of nn.Module, inherit its methods.\"\"\"\n",
        "        super(NeuralNet, self).__init__()\n",
        "\n",
        "        self.input_dims = input_dims\n",
        "        self.n_neurons = n_neurons\n",
        "        self.n_layers = n_layers\n",
        "\n",
        "        # Layer structure\n",
        "        # First layer\n",
        "        self.in_layer = nn.Linear(self.input_dims, self.n_neurons)\n",
        "\n",
        "        # Dense, Dropout, Activation and BN\n",
        "        self.dense = nn.Linear(self.n_neurons, self.n_neurons)\n",
        "        self.dropout = nn.Dropout(p=dropout_rate)\n",
        "        self.activation = activation\n",
        "        self.batchnorm = nn.BatchNorm1d(self.n_neurons)\n",
        "\n",
        "        # Output layer\n",
        "        self.out_layer = nn.Linear(self.n_neurons, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"Forward pass.\"\"\"\n",
        "                \n",
        "        x = self.batchnorm(self.activation(self.dropout(self.in_layer(x))))\n",
        "\n",
        "        for i in range(self.n_layers-1):\n",
        "            x = self.batchnorm(self.activation(self.dropout(self.dense(x))))\n",
        "\n",
        "        x = self.out_layer(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "\n",
        "def train_step(model, features, targets, optimizer, loss_fn):\n",
        "    \"\"\"Perform a single training step.\n",
        "\n",
        "    Calulcates prediction, loss and gradients for a single batch\n",
        "    and updates optimizer parameters accordingly.\"\"\"\n",
        "\n",
        "    # Set gradients to zero\n",
        "    model.zero_grad()\n",
        "    # Pass data through model\n",
        "    output = model(features)\n",
        "    # Calculate loss\n",
        "    loss = loss_fn(output, targets)\n",
        "    # Calculate gradients\n",
        "    loss.backward()\n",
        "    # Update parameters\n",
        "    optimizer.step()\n",
        "\n",
        "    return loss, output\n",
        "\n",
        "\n",
        "def train_network(model, train_data, val_data, optimizer, loss_fn,\n",
        "                  n_epochs=2000, patience=100, print_progress=True):\n",
        "    \"\"\"Train a neural network model.\"\"\"\n",
        "    # Initalize loss as very high\n",
        "    best_loss = 1e8\n",
        "\n",
        "    # Create lists to hold train and val losses\n",
        "    train_loss = []\n",
        "    val_loss = []\n",
        "    # Init epochs_no_improve\n",
        "    epochs_no_improve = 0\n",
        "    best_model = copy.deepcopy(model.state_dict())\n",
        "    \n",
        "    # Start training (loop over epochs)\n",
        "    for epoch in range(n_epochs):\n",
        "\n",
        "        # Initalize epoch train loss\n",
        "        train_epoch_loss = 0\n",
        "        # Loop over training batches\n",
        "        model.train()  # set model to training mode for training\n",
        "        for bidx, (features, targets) in enumerate(train_data):\n",
        "            # Calculate loss and predictions\n",
        "            loss, predictions = train_step(model, features, targets,\n",
        "                                           optimizer, loss_fn)\n",
        "            train_epoch_loss += loss\n",
        "        # Save train epoch loss\n",
        "        train_loss.append(train_epoch_loss.item())\n",
        "\n",
        "        # Initialize val epoch loss\n",
        "        val_epoch_loss = 0\n",
        "        # Loop over validation batches\n",
        "        model.eval()  # set model to evaluation mode for validation\n",
        "        for bidx, (features, targets) in enumerate(val_data):\n",
        "            output = model(features)\n",
        "            val_epoch_loss += loss_fn(output, targets)\n",
        "        # Save val epoch loss\n",
        "        val_loss.append(val_epoch_loss.item())\n",
        "\n",
        "        # Early stopping (check if val loss is an improvement on current best)\n",
        "        if val_epoch_loss < best_loss:\n",
        "            best_loss = val_epoch_loss.item()\n",
        "            best_model = copy.deepcopy(model.state_dict())\n",
        "            epochs_no_improve = 0\n",
        "        else:\n",
        "            epochs_no_improve += 1\n",
        "\n",
        "            # Check early stopping condition\n",
        "            if epochs_no_improve == patience:\n",
        "                print(f'Stopping after {epoch} epochs due to no improvement.')\n",
        "                model.load_state_dict(best_model)\n",
        "                break\n",
        "        # Print progress at set epoch intervals if desired\n",
        "        if print_progress:\n",
        "            if (epoch + 1) % 100 == 0:\n",
        "                print(f'Epoch {epoch+1} Train Loss: {train_epoch_loss:.4}, ', end='')\n",
        "                print(f'Val Loss: {val_epoch_loss:.4}')\n",
        "\n",
        "    return train_loss, val_loss\n",
        "\n",
        "\n",
        "def weight_reset(m):\n",
        "    \"\"\"Reset all weights in an NN.\"\"\"\n",
        "    reset_parameters = getattr(m, \"reset_parameters\", None)\n",
        "    if callable(reset_parameters):\n",
        "        m.reset_parameters()"
      ],
      "outputs": [],
      "metadata": {
        "id": "b_Y8ikgOGEc4"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "source": [
        "# ------------------- Cross-validation -------------------------------------- #\n",
        "\n",
        "def kfold_cv_train(x_train, y_train, model, optimizer,loss_fn=nn.MSELoss(), \n",
        "                   n_splits=5, batch_size=312, n_epochs=2000, patience=100,\n",
        "                   shuffle=True, rng=SEED):\n",
        "    \"\"\"Train a NN with K-Fold cross-validation.\"\"\"\n",
        "    kfold = KFold(n_splits=n_splits, shuffle=shuffle, random_state=rng)\n",
        "    best_losses = []\n",
        "\n",
        "    for fold, (train_index, val_index) in enumerate(kfold.split(x_train, y_train)):\n",
        "        # print(f'Starting fold {fold + 1}')\n",
        "        # Get training and val features\n",
        "        x_train_fold = x_train[train_index]\n",
        "        x_val_fold = x_train[val_index]\n",
        "\n",
        "        # Get training and val targets\n",
        "        y_train_fold = y_train[train_index]\n",
        "        y_val_fold = y_train[val_index]\n",
        "\n",
        "        train = Dataset(x_train_fold, y_train_fold)\n",
        "        train_loader = DataLoader(train, batch_size=batch_size,\n",
        "                                  shuffle=shuffle, drop_last=True, generator=torch.Generator(device='cuda'))\n",
        "        # Create val dataset and dataloader\n",
        "        val = Dataset(x_val_fold, y_val_fold)\n",
        "        val_loader = DataLoader(val, batch_size=batch_size,\n",
        "                                shuffle=False, drop_last=False, generator=torch.Generator(device='cuda'))\n",
        "\n",
        "        # Train\n",
        "        train_loss, val_loss = train_network(model=model,\n",
        "                                             train_data=train_loader,\n",
        "                                             val_data=val_loader,\n",
        "                                             optimizer=optimizer,\n",
        "                                             loss_fn=loss_fn,\n",
        "                                             n_epochs=n_epochs,\n",
        "                                             patience=patience,\n",
        "                                             print_progress=False)\n",
        "        best_losses.append(min(val_loss))\n",
        "        model.apply(weight_reset)\n",
        "\n",
        "    return sum(best_losses)/n_splits, train_loss, val_loss"
      ],
      "outputs": [],
      "metadata": {
        "id": "z3N81Qr1GEgi"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "source": [
        "# ------------------- Bayesian optimization --------------------------------- #\n",
        "\n",
        "\n",
        "class tqdm_skopt(object):\n",
        "    \"\"\"Progress bar object for functions with callbacks.\"\"\"\n",
        "\n",
        "    def __init__(self, **kwargs):\n",
        "        self._bar = tqdm(**kwargs)\n",
        "\n",
        "    def __call__(self, res):\n",
        "        \"\"\"Update bar with intermediate results.\"\"\"\n",
        "        self._bar.update()\n",
        "\n",
        "\n",
        "# Set parameter search space\n",
        "space = []\n",
        "# space.append(Categorical(['relu', 'leakyrelu', 'elu'], name='activation'))\n",
        "space.append(Real(1e-5, 1e-1, name='learning_rate'))\n",
        "space.append(Real(1e-10, 1e-1, name='regularization'))\n",
        "space.append(Real(0.0, 0.9, name='dropout_rate'))\n",
        "# space.append(Integer(int(16), int(312), name='batch_size', dtype=int))\n",
        "# space.append(Categorical(['relu', 'leakyrelu', 'prelu', 'elu', 'selu'], name='activation'))\n",
        "space.append(Integer(int(1), int(3), name='n_layers', dtype=int))\n",
        "space.append(Integer(int(8), int(256), name='n_neurons', dtype=int))\n",
        "\n",
        "# # Set default hyperparameters\n",
        "# default_params = [1e-3,\n",
        "#                   1e-5,\n",
        "#                   0.25,\n",
        "#                   2,\n",
        "#                   128]\n",
        "\n",
        "# Work in progress\n",
        "@use_named_args(dimensions=space)\n",
        "def fitness(learning_rate, regularization, dropout_rate, n_layers, n_neurons):\n",
        "    \"\"\"Perform Bayesian Hyperparameter tuning.\"\"\"\n",
        "\n",
        "    # if activation == 'relu':\n",
        "    #     activation = nn.ReLU()\n",
        "    # elif activation == 'leakyrelu':\n",
        "    #     activation = nn.LeakyReLU()\n",
        "    # elif activation == 'elu':\n",
        "    #     activation = nn.ELU()\n",
        "    # elif activation == 'selu':\n",
        "    #     activation = nn.SELU()\n",
        "    # elif activation == 'prelu':\n",
        "    #     activation = nn.PReLU()\n",
        "\n",
        "    # print(f'Learning Rate: {learning_rate:.0e}, Regularization: {regularization:.0e}, ', end='')\n",
        "    # print(f'Dropout: {dropout:.2f}')  #, Batch Size: {batch_size}')\n",
        "\n",
        "    model = NeuralNet(activation=nn.ELU(), dropout_rate=dropout_rate, n_neurons=n_neurons, n_layers=n_layers)\n",
        "    model.to(device)\n",
        "    optimizer = optim.AdamW(model.parameters(), lr=learning_rate,\n",
        "                                                weight_decay=regularization)\n",
        "    # Create k-fold cross validation\n",
        "    avg_best_loss, *_ = kfold_cv_train(x_train=x_train, y_train=y_train,\n",
        "                                     model=model, optimizer=optimizer,\n",
        "                                     batch_size=312, patience=100)\n",
        "    # print(f'Avg. best validation loss: {avg_best_loss}')\n",
        "\n",
        "    return avg_best_loss\n",
        "\n",
        "n_calls = 100\n",
        "# Hyperparemeter search using Bayesian optimization\n",
        "gp_result = gp_minimize(func=fitness,\n",
        "                        dimensions=space,\n",
        "                        n_calls=n_calls,\n",
        "                        n_initial_points=20,\n",
        "                        random_state=SEED,\n",
        "                        verbose=True, callback=[tqdm_skopt(total=n_calls,\n",
        "                                          desc='Gaussian Process')])\n",
        "\n",
        "plot_convergence(gp_result)\n",
        "plot_objective(gp_result)\n",
        "plot_evaluations(gp_result)\n",
        "gp_result.x"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Gaussian Process:   0%|          | 0/100 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration No: 1 started. Evaluating function at random point.\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "Expected a 'cuda' device type for generator but found 'cpu'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-dc562d01d2a1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[0mn_calls\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[0;31m# Hyperparemeter search using Gaussian process minimization\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m gp_result = gp_minimize(func=fitness,\n\u001b[0m\u001b[1;32m     67\u001b[0m                         \u001b[0mdimensions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mspace\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m                         \u001b[0mn_calls\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_calls\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/.conda/envs/geo/lib/python3.9/site-packages/skopt/optimizer/gp.py\u001b[0m in \u001b[0;36mgp_minimize\u001b[0;34m(func, dimensions, base_estimator, n_calls, n_random_starts, n_initial_points, initial_point_generator, acq_func, acq_optimizer, x0, y0, random_state, verbose, callback, n_points, n_restarts_optimizer, xi, kappa, noise, n_jobs, model_queue_size)\u001b[0m\n\u001b[1;32m    257\u001b[0m             noise=noise)\n\u001b[1;32m    258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 259\u001b[0;31m     return base_minimize(\n\u001b[0m\u001b[1;32m    260\u001b[0m         \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mspace\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbase_estimator\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbase_estimator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    261\u001b[0m         \u001b[0macq_func\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0macq_func\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/.conda/envs/geo/lib/python3.9/site-packages/skopt/optimizer/base.py\u001b[0m in \u001b[0;36mbase_minimize\u001b[0;34m(func, dimensions, base_estimator, n_calls, n_random_starts, n_initial_points, initial_point_generator, acq_func, acq_optimizer, x0, y0, random_state, verbose, callback, n_points, n_restarts_optimizer, xi, kappa, n_jobs, model_queue_size)\u001b[0m\n\u001b[1;32m    299\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mn\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_calls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    300\u001b[0m         \u001b[0mnext_x\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mask\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 301\u001b[0;31m         \u001b[0mnext_y\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext_x\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    302\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtell\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_y\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    303\u001b[0m         \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspecs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspecs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/.conda/envs/geo/lib/python3.9/site-packages/skopt/utils.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m    801\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    802\u001b[0m             \u001b[0;31m# Call the wrapped objective function with the named arguments.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 803\u001b[0;31m             \u001b[0mobjective_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0marg_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    804\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    805\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mobjective_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-7-dc562d01d2a1>\u001b[0m in \u001b[0;36mfitness\u001b[0;34m(learning_rate, regularization, dropout_rate, n_layers, n_neurons)\u001b[0m\n\u001b[1;32m     55\u001b[0m                                                 weight_decay=regularization)\n\u001b[1;32m     56\u001b[0m     \u001b[0;31m# Create k-fold cross validation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m     avg_best_loss, *_ = kfold_cv_train(x_train=x_train, y_train=y_train,\n\u001b[0m\u001b[1;32m     58\u001b[0m                                      \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m                                      batch_size=312, patience=100)\n",
            "\u001b[0;32m<ipython-input-6-d0d4d1e50740>\u001b[0m in \u001b[0;36mkfold_cv_train\u001b[0;34m(x_train, y_train, model, optimizer, loss_fn, n_splits, batch_size, n_epochs, patience, shuffle, rng)\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0;31m# Train\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m         train_loss, val_loss = train_network(model=model,\n\u001b[0m\u001b[1;32m     30\u001b[0m                                              \u001b[0mtrain_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m                                              \u001b[0mval_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_loader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-5-69bd638cf47f>\u001b[0m in \u001b[0;36mtrain_network\u001b[0;34m(model, train_data, val_data, optimizer, loss_fn, n_epochs, patience, print_progress)\u001b[0m\n\u001b[1;32m     79\u001b[0m         \u001b[0;31m# Loop over training batches\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# set model to training mode for training\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 81\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mbidx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     82\u001b[0m             \u001b[0;31m# Calculate loss and predictions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m             loss, predictions = train_step(model, features, targets,\n",
            "\u001b[0;32m~/.conda/envs/geo/lib/python3.9/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    519\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sampler_iter\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    520\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 521\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    522\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    523\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/.conda/envs/geo/lib/python3.9/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    558\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    559\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 560\u001b[0;31m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    561\u001b[0m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    562\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/.conda/envs/geo/lib/python3.9/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_index\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    510\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    511\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 512\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sampler_iter\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    513\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    514\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/.conda/envs/geo/lib/python3.9/site-packages/torch/utils/data/sampler.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    224\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__iter__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mIterator\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    225\u001b[0m         \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 226\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msampler\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    227\u001b[0m             \u001b[0mbatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    228\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/.conda/envs/geo/lib/python3.9/site-packages/torch/utils/data/sampler.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    122\u001b[0m             \u001b[0;32myield\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhigh\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_samples\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mint64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgenerator\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgenerator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 124\u001b[0;31m             \u001b[0;32myield\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandperm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgenerator\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgenerator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    125\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Expected a 'cuda' device type for generator but found 'cpu'"
          ]
        }
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "oC67K6lUEc2x",
        "outputId": "6b47d1a8-9d9e-494e-d823-aa2a135378aa"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "source": [],
      "outputs": [],
      "metadata": {
        "id": "L5GY3uJQHTSj"
      }
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "authorship_tag": "ABX9TyMPx1Fh4yteUNAoR1AolFfn",
      "include_colab_link": true,
      "name": "mlp-colab.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {}
  },
  "nbformat": 4,
  "nbformat_minor": 2
}