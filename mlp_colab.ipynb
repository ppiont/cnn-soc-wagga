{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "mlp-colab.ipynb",
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyO7/bWgos7yF9Q0ipy6tfO/",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ppiont/cnn-soc-wagga/blob/master/mlp_colab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "blbFec8MEbhF",
        "outputId": "948123fe-be4d-4938-9396-e4b2c718a3b2"
      },
      "source": [
        "pip install scikit-optimize"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting scikit-optimize\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/8b/03/be33e89f55866065a02e515c5b319304a801a9f1027a9b311a9b1d1f8dc7/scikit_optimize-0.8.1-py2.py3-none-any.whl (101kB)\n",
            "\u001b[K     |████████████████████████████████| 102kB 7.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-optimize) (1.0.1)\n",
            "Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.7/dist-packages (from scikit-optimize) (1.19.5)\n",
            "Requirement already satisfied: scipy>=0.19.1 in /usr/local/lib/python3.7/dist-packages (from scikit-optimize) (1.4.1)\n",
            "Requirement already satisfied: scikit-learn>=0.20.0 in /usr/local/lib/python3.7/dist-packages (from scikit-optimize) (0.22.2.post1)\n",
            "Collecting pyaml>=16.9\n",
            "  Downloading https://files.pythonhosted.org/packages/15/c4/1310a054d33abc318426a956e7d6df0df76a6ddfa9c66f6310274fb75d42/pyaml-20.4.0-py2.py3-none-any.whl\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.7/dist-packages (from pyaml>=16.9->scikit-optimize) (3.13)\n",
            "Installing collected packages: pyaml, scikit-optimize\n",
            "Successfully installed pyaml-20.4.0 scikit-optimize-0.8.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6OEVoc-YEoEu"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')\n",
        "%cd \"/content/drive/MyDrive/Thesis/cnn-soc-wagga\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LisxU8b2EMcJ"
      },
      "source": [
        "# Standard lib imports\n",
        "import os\n",
        "import pathlib\n",
        "import random\n",
        "\n",
        "# Imports\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from skopt import gp_minimize, dump, load\n",
        "from skopt.space import Integer, Real, Categorical\n",
        "from skopt.utils import use_named_args\n",
        "from skopt.plots import plot_objective, plot_convergence, plot_evaluations\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader\n",
        "import copy\n",
        "import pdb  # Brug det"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cbG11p-dGEVA"
      },
      "source": [
        "# ------------------- TO DO ------------------------------------------------- #\n",
        "\n",
        "\"\"\"\n",
        "Use Torch Dataset.. you made a class for it dummy\n",
        "Should I shuffle train_loader in CV?\n",
        "\"\"\"\n",
        "\n",
        "# ------------------- Settings ---------------------------------------------- #\n",
        "\n",
        "\n",
        "# Set matploblib style\n",
        "plt.style.use('seaborn-colorblind')\n",
        "colors = plt.rcParams['axes.prop_cycle'].by_key()['color']\n",
        "plt.rcParams['figure.dpi'] = 450\n",
        "plt.rcParams['savefig.transparent'] = True\n",
        "plt.rcParams['savefig.format'] = 'svg'\n",
        "\n",
        "# Reset params if needed\n",
        "# plt.rcParams.update(mpl.rcParamsDefault)\n",
        "\n",
        "\n",
        "# ------------------- Organization ------------------------------------------ #\n",
        "\n",
        "\n",
        "DATA_DIR = pathlib.Path('data/')\n",
        "\n",
        "\n",
        "def seed_everything(SEED=43):\n",
        "    random.seed(SEED)\n",
        "    np.random.seed(SEED)\n",
        "    torch.manual_seed(SEED)\n",
        "    torch.cuda.manual_seed(SEED)\n",
        "    torch.cuda.manual_seed_all(SEED)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    os.environ['PYTHONHASHSEED']=str(SEED)\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "\n",
        "\n",
        "SEED = 43\n",
        "seed_everything(SEED=SEED)\n",
        "\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "torch.set_default_tensor_type('torch.cuda.FloatTensor')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FjTB4-GdGEZD"
      },
      "source": [
        "# ------------------- Read and prep data ------------------------------------ #\n",
        "\n",
        "train_data = np.load(DATA_DIR.joinpath('train_no_outlier.npy'))\n",
        "test_data = np.load(DATA_DIR.joinpath('test_no_outlier.npy'))\n",
        "\n",
        "x_train = train_data[:, 1:]\n",
        "y_train = train_data[:, 0]\n",
        "\n",
        "input_shape=x_train.shape[-1]\n",
        "\n",
        "x_test = test_data[:, 1:]\n",
        "y_test = test_data[:, 0]\n",
        "\n",
        "# Normalize X\n",
        "scaler_x = MinMaxScaler()\n",
        "scaler_x.fit(x_train)\n",
        "x_train = scaler_x.transform(x_train)\n",
        "x_test = scaler_x.transform(x_test)\n",
        "\n",
        "# Normalize y\n",
        "scaler_y = MinMaxScaler()\n",
        "scaler_y.fit(y_train.reshape(-1, 1))\n",
        "y_train = scaler_y.fit_transform(y_train.reshape(-1, 1))\n",
        "# There is no reason to scale y_test actually\n",
        "y_test = scaler_y.transform(y_test.reshape(-1, 1))\n",
        "\n",
        "\n",
        "x_train, y_train = torch.from_numpy(x_train), torch.from_numpy(y_train)\n",
        "x_test, y_test = torch.from_numpy(x_test), torch.from_numpy(y_test)\n",
        "\n",
        "x_train, x_test = x_train.to(device), x_test.to(device)\n",
        "y_train, y_test = y_train.to(device), y_test.to(device)\n",
        "\n",
        "class Dataset(torch.utils.data.TensorDataset):\n",
        "    \"\"\"Characterize a PyTorch Dataset.\"\"\"\n",
        "\n",
        "    def __init__(self, features, targets):\n",
        "        \"\"\"Initialize with X and y.\"\"\"\n",
        "        self.features = features\n",
        "        self.targets = targets\n",
        "\n",
        "    def __len__(self):\n",
        "        \"\"\"Return total number of samples.\"\"\"\n",
        "        return len(self.targets)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        \"\"\"Generate one data sample.\"\"\"\n",
        "        return self.features[index], self.targets[index]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b_Y8ikgOGEc4"
      },
      "source": [
        "# ------------------- NN setup ---------------------------------------------- #\n",
        "\n",
        "class NeuralNet(nn.Module):\n",
        "    \"\"\"Neural Network class.\"\"\"\n",
        "\n",
        "    def __init__(self, input_dims=input_dims, n_layers=3, n_neurons=64,\n",
        "                 activation=nn.ELU(), dropout_rate=0.5):\n",
        "        \"\"\"Initialize as subclass of nn.Module, inherit its methods.\"\"\"\n",
        "        super(NeuralNet, self).__init__()\n",
        "\n",
        "        self.input_dims = input_dims\n",
        "        self.n_neurons = n_neurons\n",
        "        self.n_layers = n_layers\n",
        "\n",
        "        # Layer structure\n",
        "        # First layer\n",
        "        self.in_layer = nn.Linear(self.input_dims, self.n_neurons)\n",
        "\n",
        "        # Dense, Dropout, Activation and BN\n",
        "        self.dense = nn.Linear(self.n_neurons, self.n_neurons)\n",
        "        self.dropout = nn.Dropout(p=dropout_rate)\n",
        "        self.activation = activation\n",
        "        self.batchnorm = nn.BatchNorm1d(self.n_neurons)\n",
        "\n",
        "        # Output layer\n",
        "        self.out_layer = nn.Linear(self.n_neurons, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"Forward pass.\"\"\"\n",
        "                \n",
        "        x = self.batchnorm(self.activation(self.dropout(self.in_layer(x))))\n",
        "\n",
        "        for i in range(self.n_layers-1):\n",
        "            x = self.batchnorm(self.activation(self.dropout(self.dense(x))))\n",
        "\n",
        "        x = self.out_layer(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "\n",
        "def train_step(model, features, targets, optimizer, loss_fn):\n",
        "    \"\"\"Perform a single training step.\n",
        "\n",
        "    Calulcates prediction, loss and gradients for a single batch\n",
        "    and updates optimizer parameters accordingly.\"\"\"\n",
        "\n",
        "    # Set gradients to zero\n",
        "    model.zero_grad()\n",
        "    # Pass data through model\n",
        "    output = model(features)\n",
        "    # Calculate loss\n",
        "    loss = loss_fn(output, targets)\n",
        "    # Calculate gradients\n",
        "    loss.backward()\n",
        "    # Update parameters\n",
        "    optimizer.step()\n",
        "\n",
        "    return loss, output\n",
        "\n",
        "\n",
        "def train_network(model, train_data, val_data, optimizer, loss_fn,\n",
        "                  n_epochs=2000, patience=100, print_progress=True):\n",
        "    \"\"\"Train a neural network model.\"\"\"\n",
        "    # Initalize loss as very high\n",
        "    best_loss = 1e8\n",
        "\n",
        "    # Create lists to hold train and val losses\n",
        "    train_loss = []\n",
        "    val_loss = []\n",
        "    # Init epochs_no_improve\n",
        "    epochs_no_improve = 0\n",
        "    # best_model = copy.deepcopy(model.state_dict())\n",
        "    \n",
        "    # Start training (loop over epochs)\n",
        "    for epoch in range(n_epochs):\n",
        "\n",
        "        # Initalize epoch train loss\n",
        "        train_epoch_loss = 0\n",
        "        # Loop over training batches\n",
        "        model.train()  # set model to training mode for training\n",
        "        for bidx, (features, targets) in enumerate(train_data):\n",
        "            # Calculate loss and predictions\n",
        "            loss, predictions = train_step(model, features, targets,\n",
        "                                           optimizer, loss_fn)\n",
        "            train_epoch_loss += loss\n",
        "        # Save train epoch loss\n",
        "        train_loss.append(train_epoch_loss.item())\n",
        "\n",
        "        # Initialize val epoch loss\n",
        "        val_epoch_loss = 0\n",
        "        # Loop over validation batches\n",
        "        model.eval()  # set model to evaluation mode for validation\n",
        "        for bidx, (features, targets) in enumerate(val_data):\n",
        "            output = model(features)\n",
        "            val_epoch_loss += loss_fn(output, targets)\n",
        "        # Save val epoch loss\n",
        "        val_loss.append(val_epoch_loss.item())\n",
        "\n",
        "        # Early stopping (check if val loss is an improvement on current best)\n",
        "        if val_epoch_loss < best_loss:\n",
        "            best_loss = val_epoch_loss.item()\n",
        "            best_model = copy.deepcopy(model.state_dict())\n",
        "            epochs_no_improve = 0\n",
        "        else:\n",
        "            epochs_no_improve += 1\n",
        "\n",
        "            # Check early stopping condition\n",
        "            if epochs_no_improve == patience:\n",
        "                print(f'Stopping after {epoch} epochs due to no improvement.')\n",
        "                model.load_state_dict(best_model)\n",
        "                break\n",
        "        # Print progress at set epoch intervals if desired\n",
        "        if print_progress:\n",
        "            if (epoch + 1) % 100 == 0:\n",
        "                print(f'Epoch {epoch+1} Train Loss: {train_epoch_loss:.4}, ', end='')\n",
        "                print(f'Val Loss: {val_epoch_loss:.4}')\n",
        "\n",
        "    return train_loss, val_loss\n",
        "\n",
        "\n",
        "def weight_reset(m):\n",
        "    \"\"\"Reset all weights in an NN.\"\"\"\n",
        "    reset_parameters = getattr(m, \"reset_parameters\", None)\n",
        "    if callable(reset_parameters):\n",
        "        m.reset_parameters()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z3N81Qr1GEgi"
      },
      "source": [
        "# ------------------- Cross-validation -------------------------------------- #\n",
        "\n",
        "def kfold_cv_train(x_train, y_train, model, optimizer,loss_fn=nn.MSELoss(), \n",
        "                   n_splits=5, batch_size=312, n_epochs=2000, patience=100,\n",
        "                   shuffle=True, rng=SEED):\n",
        "    \"\"\"Train a NN with K-Fold cross-validation.\"\"\"\n",
        "    kfold = KFold(n_splits=n_splits, shuffle=shuffle, random_state=rng)\n",
        "    best_losses = []\n",
        "\n",
        "    for fold, (train_index, val_index) in enumerate(kfold.split(x_train, y_train)):\n",
        "        # print(f'Starting fold {fold + 1}')\n",
        "        # Get training and val features\n",
        "        x_train_fold = x_train[train_index]\n",
        "        x_val_fold = x_train[val_index]\n",
        "\n",
        "        # Get training and val targets\n",
        "        y_train_fold = y_train[train_index]\n",
        "        y_val_fold = y_train[val_index]\n",
        "\n",
        "        train = Dataset(x_train_fold, y_train_fold)\n",
        "        train_loader = DataLoader(train, batch_size=batch_size,\n",
        "                                  shuffle=shuffle, drop_last=True)\n",
        "        # Create val dataset and dataloader\n",
        "        val = Dataset(x_val_fold, y_val_fold)\n",
        "        val_loader = DataLoader(val, batch_size=batch_size,\n",
        "                                shuffle=False, drop_last=False)\n",
        "\n",
        "        # Train\n",
        "        train_loss, val_loss = train_network(model=model,\n",
        "                                             train_data=train_loader,\n",
        "                                             val_data=val_loader,\n",
        "                                             optimizer=optimizer,\n",
        "                                             loss_fn=loss_fn,\n",
        "                                             n_epochs=n_epochs,\n",
        "                                             patience=patience,\n",
        "                                             print_progress=False)\n",
        "        best_losses.append(min(val_loss))\n",
        "        model.apply(weight_reset)\n",
        "\n",
        "    return sum(best_losses)/n_splits, train_loss, val_loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oC67K6lUEc2x"
      },
      "source": [
        "# ------------------- Bayesian optimization --------------------------------- #\n",
        "\n",
        "\n",
        "class tqdm_skopt(object):\n",
        "    \"\"\"Progress bar object for functions with callbacks.\"\"\"\n",
        "\n",
        "    def __init__(self, **kwargs):\n",
        "        self._bar = tqdm(**kwargs)\n",
        "\n",
        "    def __call__(self, res):\n",
        "        \"\"\"Update bar with intermediate results.\"\"\"\n",
        "        self._bar.update()\n",
        "\n",
        "\n",
        "# Set parameter search space\n",
        "space = []\n",
        "# space.append(Categorical(['relu', 'leakyrelu', 'elu'], name='activation'))\n",
        "space.append(Real(1e-5, 1e-1, name='learning_rate'))\n",
        "space.append(Real(1e-10, 1e-1, name='regularization'))\n",
        "space.append(Real(0.0, 0.9, name='dropout_rate'))\n",
        "space.append(Integer(int(32), int(312), name='batch_size', dtype=int))\n",
        "space.append(Categorical(['relu', 'leakyrelu', 'prelu', 'elu', 'selu'], name='activation'))\n",
        "space.append(Integer(int(1), int(5), name='n_layers', dtype=int))\n",
        "space.append(Integer(int(16), int(512), name='n_neurons', dtype=int))\n",
        "\n",
        "# Set default hyperparameters\n",
        "default_params = [1e-3,\n",
        "                  1e-5,\n",
        "                  0.5,\n",
        "                  312,\n",
        "                  'elu',\n",
        "                  3,\n",
        "                  256]\n",
        "\n",
        "# Best params from first optimization\n",
        "# best_params = ['elu',\n",
        "#                 0.018548744510205464,\n",
        "#                 1e-10,\n",
        "#                 0.01698162022248246]\n",
        "\n",
        "# best_params1 = [0.0012304697066411964,\n",
        "#                 1.0421441915334575e-09,\n",
        "#                 0.02539150096227823]\n",
        "\n",
        "# best_params2 = [0.005315802234314809,\n",
        "#                 0.1,\n",
        "#                 0.0,\n",
        "#                 312]\n",
        "\n",
        "# best_params_colab = [0.0005877430097476587\n",
        "#                      0.1,\n",
        "#                      0.10470590080305037,\n",
        "#                      312]\n",
        "\n",
        "# Work in progress\n",
        "@use_named_args(dimensions=space)\n",
        "def fitness(learning_rate, regularization, dropout_rate, batch_size, activation,\n",
        "            n_layers, n_neurons):\n",
        "    \"\"\"Perform Bayesian Hyperparameter tuning.\"\"\"\n",
        "\n",
        "    if activation == 'relu':\n",
        "        activation = nn.ReLU()\n",
        "    elif activation == 'leakyrelu':\n",
        "        activation = nn.LeakyReLU()\n",
        "    elif activation == 'elu':\n",
        "        activation = nn.ELU()\n",
        "    elif activation == 'selu':\n",
        "        activation = nn.SELU()\n",
        "    elif activation == 'prelu':\n",
        "        activation = nn.PReLU()\n",
        "\n",
        "    # print(f'Learning Rate: {learning_rate:.0e}, Regularization: {regularization:.0e}, ', end='')\n",
        "    # print(f'Dropout: {dropout:.2f}')  #, Batch Size: {batch_size}')\n",
        "\n",
        "    model = NeuralNet(activation=activation, dropout_rate=dropout_rate,\n",
        "                      n_layers=n_layers, n_neurons=n_neurons)\n",
        "    model.to(device)\n",
        "    optimizer = optim.AdamW(model.parameters(), lr=learning_rate,\n",
        "                                                weight_decay=regularization)\n",
        "    # Create k-fold cross validation\n",
        "    avg_best_loss, *_ = kfold_cv_train(x_train=x_train, y_train=y_train,\n",
        "                                     model=model, optimizer=optimizer,\n",
        "                                     batch_size=batch_size)\n",
        "    # print(f'Avg. best validation loss: {avg_best_loss}')\n",
        "\n",
        "    return avg_best_loss\n",
        "\n",
        "n_calls = 200\n",
        "# Hyperparemeter search using Gaussian process minimization\n",
        "gp_result = gp_minimize(func=fitness,\n",
        "                        x0=default_params,\n",
        "                        dimensions=space,\n",
        "                        n_calls=n_calls,\n",
        "                        verbose=True, callback=[tqdm_skopt(total=n_calls,\n",
        "                                          desc='Gaussian Process')])\n",
        "\n",
        "plot_convergence(gp_result)\n",
        "plot_objective(gp_result)\n",
        "plot_evaluations(gp_result)\n",
        "gp_result.x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L5GY3uJQHTSj"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}