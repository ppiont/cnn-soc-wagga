#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Created on Tue Dec  1 11:32:05 2020.

@author: peter
"""
import pathlib

import numpy as np
import pandas as pd
import geopandas as gpd
from sklearn.model_selection import train_test_split
from skopt.learning import RandomForestRegressor
from skopt import BayesSearchCV
from skopt.space import Integer, Categorical  # , Real
from tqdm import tqdm
from sklearn.preprocessing import MinMaxScaler
from sklearn.model_selection import KFold, RepeatedKFold
from skopt.plots import plot_objective, plot_histogram, plot_evaluations
from skopt import dump, load
import matplotlib as mpl
import matplotlib.pyplot as plt

from feat_eng.funcs import get_corr_feats
# from custom_metrics.metrics import mean_error, lin_ccc  # custom
# from feat_eng.funcs import min_max  # custom


# ------------------- Options ----------------------------------------------- #

plt.style.use('tableau-colorblind10')
mpl.rcParams.update({"lines.linewidth": 1, "font.family": "serif",
                     "xtick.labelsize": "small", "ytick.labelsize": "small",
                     "xtick.major.size": 0, "xtick.minor.size": 0,
                     "ytick.major.size": 0, "ytick.minor.size": 0,
                     "axes.titlesize": "medium", "figure.titlesize": "medium",
                     "figure.figsize": (5, 5), "figure.dpi": 450,
                     "figure.autolayout": True, "savefig.format": "svg",
                     "savefig.transparent": True, "image.cmap": "cool"})

# # Reset params if needed
mpl.rcParams.update(mpl.rcParamsDefault)

# ------------------- Organization ------------------------------------------ #

data_dir = pathlib.Path('data/')
optimizer_dir = pathlib.Path('optimizers/')
SEED = 43

# ------------------- Read and prep data ------------------------------------ #

# Load target data
target_data = gpd.read_file(data_dir.joinpath('germany_targets.geojson'),
                            driver="GeoJSON")
# Get target array
targets = target_data.OC.values
# Load feature array
features = np.load(data_dir.joinpath('numerical_feats.npy'))
# Get the center pixel (along axes=(1, 2))
features = features[:, features.shape[1]//2, features.shape[2]//2, :]
# Split into train and test data
x_train, x_test, y_train, y_test = train_test_split(features, targets,
                                                    test_size=0.1,
                                                    random_state=SEED)

# Identify features with 0 variance
zero_var_idx = np.where(np.var(x_train, axis=0) == 0)[0]
# Remove features with 0 variance
x_train = np.delete(x_train, zero_var_idx, -1)
x_test = np.delete(x_test, zero_var_idx, -1)

# Identify features with high correlation
high_corr_idx = get_corr_feats(x_train, min_corr=0.8)
# Remove features with high correlation
x_train = np.delete(x_train, high_corr_idx, -1)
x_test = np.delete(x_test, high_corr_idx, -1)

# Normalize data
scaler = MinMaxScaler()
scaler.fit(x_train)
x_train = scaler.transform(x_train)
x_test = scaler.transform(x_test)

# Convert data to float32
x_train = x_train.astype(np.float32)
y_train = y_train.astype(np.float32)
x_test = x_test.astype(np.float32)
y_test = y_test.astype(np.float32)

# print('Training Features Shape:', x_train.shape)
# print('Training Targets Shape:', y_train.shape)
# print('Testing Features Shape:', x_test.shape)
# print('Testing Targets Shape:', y_test.shape)

# ------------------- Random Forest ----------------------------------------- #


# Define progress monitoring object
class tqdm_skopt(object):
    def __init__(self, **kwargs):
        self._bar = tqdm(**kwargs)

    def __call__(self, res):
        self._bar.update()


# Define estimator
rf = RandomForestRegressor(n_estimators=500, n_jobs=-1, random_state=SEED)

# Define search space
n_features = x_train.shape[-1]
space = dict()
# space['n_estimators'] = Integer(1000, 2500)
# space['criterion'] = Categorical(['mse', 'mae'])
space['max_features'] = Integer(1, n_features)
space['max_depth'] = Integer(10, 200)
# space['min_samples_split'] = Integer(2, 100)
# space['min_samples_leaf'] = Integer(1, 100)


# Define optimizer
n_calls = 50
cv = KFold(n_splits=5, shuffle=True, random_state=SEED)
opt = BayesSearchCV(estimator=rf, search_spaces=space, n_iter=n_calls,
                    scoring='neg_mean_squared_error', n_jobs=-1, iid=False,
                    cv=cv, random_state=SEED)

# Fit optimizer
opt.fit(x_train, y_train, callback=[tqdm_skopt(total=n_calls,
                                               desc="Bayesian Search")])

# # Save optimizer
# dump(opt, 'optimizers/RF_opt5.pkl')
# # Save results
# opt_results = pd.DataFrame(opt.cv_results_)
# opt_results.to_csv('optimizers/BSCV_5.csv')
# # Plot results
# plot_objective(opt.optimizer_results_[0])
# plot_evaluations(opt.optimizer_results_[0])
