{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "mlp-colab.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {}
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ppiont/cnn-soc-wagga/blob/master/cnn_colab_new.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "blbFec8MEbhF",
        "outputId": "4bb4966a-5186-4221-ac2a-2dd568d7d949"
      },
      "source": [
        "pip install scikit-optimize"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: scikit-optimize in /usr/local/lib/python3.7/dist-packages (0.8.1)\n",
            "Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.7/dist-packages (from scikit-optimize) (1.19.5)\n",
            "Requirement already satisfied: pyaml>=16.9 in /usr/local/lib/python3.7/dist-packages (from scikit-optimize) (21.8.3)\n",
            "Requirement already satisfied: scikit-learn>=0.20.0 in /usr/local/lib/python3.7/dist-packages (from scikit-optimize) (0.22.2.post1)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-optimize) (1.0.1)\n",
            "Requirement already satisfied: scipy>=0.19.1 in /usr/local/lib/python3.7/dist-packages (from scikit-optimize) (1.4.1)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.7/dist-packages (from pyaml>=16.9->scikit-optimize) (3.13)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6OEVoc-YEoEu",
        "outputId": "d541a40f-c301-4d3c-d661-d89b2bc474db"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')\n",
        "%cd \"/content/drive/MyDrive/Thesis/cnn-soc-wagga\""
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive/; to attempt to forcibly remount, call drive.mount(\"/content/drive/\", force_remount=True).\n",
            "/content/drive/MyDrive/Thesis/cnn-soc-wagga\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LisxU8b2EMcJ"
      },
      "source": [
        "# Standard lib imports\n",
        "import os\n",
        "import pathlib\n",
        "import random\n",
        "\n",
        "# Imports\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import MinMaxScaler, OneHotEncoder, RobustScaler, FunctionTransformer\n",
        "from sklearn.base import BaseEstimator, TransformerMixin\n",
        "from skopt import gp_minimize\n",
        "from skopt.space import Integer, Real, Categorical\n",
        "from skopt.utils import use_named_args\n",
        "from skopt.plots import plot_objective, plot_convergence, plot_evaluations\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "# import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader\n",
        "import copy\n",
        "\n",
        "# Custom imports\n",
        "# from feat_eng.funcs import add_min, safe_log, get_corr_feats, min_max\n",
        "from custom_metrics.metrics import mean_error, lin_ccc, model_efficiency_coefficient"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cbG11p-dGEVA"
      },
      "source": [
        "# ------------------- TO DO ------------------------------------------------- #\n",
        "\n",
        "\"\"\"\n",
        "Use Torch Dataset.. you made a class for it dummy\n",
        "\"\"\"\n",
        "\n",
        "# ------------------- Settings ---------------------------------------------- #\n",
        "\n",
        "\n",
        "# Set matploblib style\n",
        "plt.style.use('seaborn-colorblind')\n",
        "colors = plt.rcParams['axes.prop_cycle'].by_key()['color']\n",
        "plt.rcParams['figure.dpi'] = 450\n",
        "plt.rcParams['savefig.transparent'] = True\n",
        "plt.rcParams['savefig.format'] = 'svg'\n",
        "\n",
        "# Reset params if needed\n",
        "# plt.rcParams.update(mpl.rcParamsDefault)\n",
        "\n",
        "\n",
        "# ------------------- Organization ------------------------------------------ #\n",
        "\n",
        "\n",
        "DATA_DIR = pathlib.Path('data/')\n",
        "\n",
        "\n",
        "def seed_everything(SEED=43):\n",
        "    random.seed(SEED)\n",
        "    np.random.seed(SEED)\n",
        "    torch.manual_seed(SEED)\n",
        "    torch.cuda.manual_seed(SEED)\n",
        "    torch.cuda.manual_seed_all(SEED)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    os.environ['PYTHONHASHSEED'] = str(SEED)\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "\n",
        "\n",
        "SEED = 43\n",
        "seed_everything(SEED=SEED)\n",
        "\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "torch.set_default_tensor_type('torch.cuda.FloatTensor')"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FjTB4-GdGEZD"
      },
      "source": [
        "# ------------------- Read and prep data ------------------------------------ #\n",
        "\n",
        "\n",
        "class Dataset(torch.utils.data.TensorDataset):\n",
        "    \"\"\"Characterize a PyTorch Dataset.\"\"\"\n",
        "\n",
        "    def __init__(self, features, targets):\n",
        "        super().__init__()\n",
        "\n",
        "        \"\"\"Initialize with X and y.\"\"\"\n",
        "        self.features = torch.from_numpy(features).permute(0, 3, 1, 2)\n",
        "        self.targets = torch.from_numpy(targets)\n",
        "\n",
        "    def __len__(self):\n",
        "        \"\"\"Return total number of samples.\"\"\"\n",
        "        return len(self.targets)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        \"\"\"Generate one data sample.\"\"\"\n",
        "\n",
        "        if torch.is_tensor(idx):\n",
        "            idx = idx.tolist()\n",
        "\n",
        "        return self.features[idx].to(device), self.targets[idx].to(device)\n",
        "\n",
        "features = np.load(\"data/cnn_features.npy\")\n",
        "targets = np.load(\"data/cnn_targets.npy\")[:, 0]\n",
        "\n",
        "\n",
        "# Train-val-test split 7-2-1\n",
        "x_train_, x_test, y_train_, y_test = train_test_split(features, targets, test_size=2 / 10)\n",
        "x_train, x_val, y_train, y_val = train_test_split(x_train_, y_train_, test_size=2 / 8)\n",
        "\n",
        "feature_reshaper = FunctionTransformer(\n",
        "    func=np.reshape,\n",
        "    inverse_func=np.reshape,\n",
        "    kw_args={\"newshape\": (-1, 43)},\n",
        "    inv_kw_args={\"newshape\": (-1, 15, 15, 43)},\n",
        ")\n",
        "feature_inverse_reshaper = FunctionTransformer(func=np.reshape, kw_args={\"newshape\": (-1, 15, 15, 43)})\n",
        "\n",
        "target_reshaper = FunctionTransformer(func=np.reshape, kw_args={\"newshape\": (-1, 1)})\n",
        "\n",
        "# Preprocessing\n",
        "feature_transformer = Pipeline(\n",
        "    steps=[\n",
        "        (\"reshaper\", feature_reshaper),\n",
        "        (\"minmax_scaler\", MinMaxScaler()),\n",
        "        (\"inverse_reshaper\", feature_inverse_reshaper),\n",
        "    ]\n",
        ")\n",
        "target_transformer = Pipeline(steps=[(\"reshaper\", target_reshaper), (\"minmax_scaler\", MinMaxScaler())])\n",
        "\n",
        "kfold_data = Dataset(feature_transformer.fit_transform(x_train_), target_transformer.fit_transform(y_train_))\n",
        "# train_data = Dataset(feature_transformer.fit_transform(x_train), target_transformer.fit_transform(y_train))\n",
        "# val_data = Dataset(feature_transformer.transform(x_val), target_transformer.transform(y_val))\n",
        "# test_data = Dataset(feature_transformer.transform(x_test), target_transformer.transform(y_test))\n",
        "\n",
        "# batch_size = 128\n",
        "\n",
        "# train_loader = DataLoader(train_data, batch_size=batch_size, num_workers=2, pin_memory=False)\n",
        "# val_loader = DataLoader(val_data, batch_size=batch_size, num_workers=2, pin_memory=False)\n",
        "# test_loader = DataLoader(test_data, batch_size=batch_size, num_workers=2, pin_memory=False)"
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b_Y8ikgOGEc4"
      },
      "source": [
        "# ------------------ CNN setup ---------------------------------------------- #\n",
        "\n",
        "\n",
        "class CNN(nn.Module):\n",
        "    \"\"\"Neural Network class.\"\"\"\n",
        "\n",
        "    def __init__(self, conv1_channels=32, conv2_channels=64, linear1_neurons=64, linear2_neurons=32):\n",
        "        \"\"\"Initialize as subclass of nn.Module, inherit its methods.\"\"\"\n",
        "        super().__init__()\n",
        "\n",
        "        self.conv1_channels = conv1_channels\n",
        "        self.conv2_channels = conv2_channels\n",
        "        self.linear1_neurons = linear1_neurons\n",
        "        self.linear2_neurons = linear2_neurons\n",
        "\n",
        "        self.conv1 = nn.Conv2d(43, self.conv1_channels, kernel_size=3, stride=3)  # stride i stedet for maxpool\n",
        "        self.relu = nn.ReLU()\n",
        "        self.bn1 = nn.BatchNorm2d(self.conv1_channels)\n",
        "        self.conv2 = nn.Conv2d(self.conv1_channels, self.conv2_channels, 3)\n",
        "        self.bn2 = nn.BatchNorm2d(self.conv2_channels)\n",
        "        self.flat = nn.Flatten()\n",
        "        self.fc1 = nn.LazyLinear(self.linear1_neurons)\n",
        "        self.bn3 = nn.BatchNorm1d(self.linear1_neurons)\n",
        "        self.fc2 = nn.Linear(self.linear1_neurons, self.linear2_neurons)\n",
        "        self.bn4 = nn.BatchNorm1d(self.linear2_neurons)\n",
        "        self.out = nn.Linear(self.linear2_neurons, 1)\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        x.to(device)\n",
        "        x = self.conv1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.bn1(x)\n",
        "        x = self.conv2(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.bn2(x)\n",
        "        x = self.flat(x)\n",
        "        x = self.fc1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.bn3(x)\n",
        "        x = self.fc2(x)\n",
        "        x = self.bn4(x)\n",
        "        x = self.out(x)\n",
        "\n",
        "        return x  # prediction\n",
        "\n",
        "\n",
        "def train_step(model, features, targets, optimizer, loss_fn):\n",
        "    \"\"\"Perform a single training step.\n",
        "\n",
        "    Calulcates prediction, loss and gradients for a single batch\n",
        "    and updates optimizer parameters accordingly.\"\"\"\n",
        "\n",
        "    # Set gradients to zero\n",
        "    model.zero_grad()\n",
        "    # Pass data through model\n",
        "    output = model(features)\n",
        "    # Calculate loss\n",
        "    loss = loss_fn(output, targets)\n",
        "    # Calculate gradients\n",
        "    loss.backward()\n",
        "    # Update parameters\n",
        "    optimizer.step()\n",
        "\n",
        "    return loss, output\n",
        "\n",
        "\n",
        "def train_network(model, train_data, val_data, optimizer, loss_fn, n_epochs=2000, patience=100, print_progress=True):\n",
        "    \"\"\"Train a neural network model.\"\"\"\n",
        "    # Initalize loss as very high\n",
        "    best_loss = 1e8\n",
        "\n",
        "    # Create lists to hold train and val losses\n",
        "    train_loss = []\n",
        "    val_loss = []\n",
        "    # Init epochs_no_improve\n",
        "    epochs_no_improve = 0\n",
        "    # best_model = copy.deepcopy(model.state_dict())\n",
        "\n",
        "    # Start training (loop over epochs)\n",
        "    for epoch in range(n_epochs):\n",
        "\n",
        "        # Initalize epoch train loss\n",
        "        train_epoch_loss = 0\n",
        "        # Loop over training batches\n",
        "        model.train()  # set model to training mode for training\n",
        "        for bidx, (features, targets) in enumerate(train_data):\n",
        "            # Calculate loss and predictions\n",
        "            loss, predictions = train_step(model, features, targets, optimizer, loss_fn)\n",
        "            train_epoch_loss += loss\n",
        "        # Save train epoch loss\n",
        "        train_loss.append(train_epoch_loss.item())\n",
        "\n",
        "        # Initialize val epoch loss\n",
        "        val_epoch_loss = 0\n",
        "        # Loop over validation batches\n",
        "        model.eval()  # set model to evaluation mode for validation\n",
        "        for bidx, (features, targets) in enumerate(val_data):\n",
        "            output = model(features)\n",
        "            val_epoch_loss += loss_fn(output, targets)\n",
        "        # Save val epoch loss\n",
        "        val_loss.append(val_epoch_loss.item())\n",
        "\n",
        "        # Early stopping (check if val loss is an improvement on current best)\n",
        "        if val_epoch_loss < best_loss:\n",
        "            best_loss = val_epoch_loss.item()\n",
        "            best_model = copy.deepcopy(model.state_dict())\n",
        "            epochs_no_improve = 0\n",
        "        else:\n",
        "            epochs_no_improve += 1\n",
        "\n",
        "            # Check early stopping condition\n",
        "            if epochs_no_improve == patience:\n",
        "                print(f\"Stopping after {epoch} epochs due to no improvement.\")\n",
        "                model.load_state_dict(best_model)\n",
        "                break\n",
        "        # Print progress at set epoch intervals if desired\n",
        "        if print_progress and (epoch + 1) % 10 == 0:\n",
        "            print(f\"Epoch {epoch+1} Train Loss: {train_epoch_loss:.4}, \", end=\"\")\n",
        "            print(f\"Val Loss: {val_epoch_loss:.4}\")\n",
        "\n",
        "    return train_loss, val_loss\n",
        "\n",
        "\n",
        "def weight_reset(m):\n",
        "    \"\"\"Reset all weights in an NN.\"\"\"\n",
        "    reset_parameters = getattr(m, \"reset_parameters\", None)\n",
        "    if callable(reset_parameters):\n",
        "        m.reset_parameters()"
      ],
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z3N81Qr1GEgi"
      },
      "source": [
        "# ------------------- Cross-validation -------------------------------------- #\n",
        "\n",
        "\n",
        "def kfold_cv_train(\n",
        "    dataset,\n",
        "    model,\n",
        "    optimizer,\n",
        "    loss_fn=nn.MSELoss(),\n",
        "    n_splits=5,\n",
        "    batch_size=128,\n",
        "    n_epochs=2000,\n",
        "    patience=100,\n",
        "    shuffle=True,\n",
        "    rng=SEED,\n",
        "):\n",
        "    \"\"\"Train a NN with K-Fold cross-validation.\"\"\"\n",
        "    kfold = KFold(n_splits=n_splits, shuffle=shuffle, random_state=rng)\n",
        "    best_losses = []\n",
        "\n",
        "    for fold, (train_ids, val_ids) in enumerate(kfold.split(dataset)):\n",
        "\n",
        "        # Print\n",
        "        print(f\"FOLD {fold}\")\n",
        "        print(\"--------------------------------\")\n",
        "\n",
        "        # Sample elements randomly from a given list of ids, no replacement.\n",
        "        train_subsampler = torch.utils.data.SubsetRandomSampler(train_ids)\n",
        "        val_subsampler = torch.utils.data.SubsetRandomSampler(val_ids)\n",
        "\n",
        "        # Define data loaders for training and testing data in this fold\n",
        "        train_loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, sampler=train_subsampler)\n",
        "        val_loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, sampler=val_subsampler)\n",
        "\n",
        "        # Train\n",
        "        train_loss, val_loss = train_network(\n",
        "            model=model,\n",
        "            train_data=train_loader,\n",
        "            val_data=val_loader,\n",
        "            optimizer=optimizer,\n",
        "            loss_fn=loss_fn,\n",
        "            n_epochs=n_epochs,\n",
        "            patience=patience,\n",
        "            print_progress=False,\n",
        "        )\n",
        "        best_losses.append(min(val_loss))\n",
        "        model.apply(weight_reset)\n",
        "\n",
        "    return sum(best_losses) / n_splits, train_loss, val_loss"
      ],
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oC67K6lUEc2x",
        "outputId": "1b61da4c-088d-4791-b11c-508d9e6d2b27"
      },
      "source": [
        "# ------------------- Bayesian optimization --------------------------------- #\n",
        "\n",
        "\n",
        "class tqdm_skopt(object):\n",
        "    \"\"\"Progress bar object for functions with callbacks.\"\"\"\n",
        "\n",
        "    def __init__(self, **kwargs):\n",
        "        self._bar = tqdm(**kwargs)\n",
        "\n",
        "    def __call__(self, res):\n",
        "        \"\"\"Update bar with intermediate results.\"\"\"\n",
        "        self._bar.update()\n",
        "\n",
        "\n",
        "# Set parameter search space\n",
        "# sourcery skip: merge-list-append\n",
        "space = []\n",
        "space.append(Real(1e-5, 1e-1, name=\"learning_rate\"))\n",
        "space.append(Real(1e-10, 1e-1, name=\"regularization\"))\n",
        "space.append(Integer(16, 128, name=\"conv1_channels\"))\n",
        "space.append(Integer(16, 128, name=\"conv2_channels\"))\n",
        "space.append(Integer(16, 128, name=\"linear1_neurons\"))\n",
        "space.append(Integer(16, 128, name=\"linear2_neurons\"))\n",
        "\n",
        "# Set default hyperparameters\n",
        "default_params = [1e-3, 1e-5, 32, 64, 64, 32]\n",
        "\n",
        "batch_size = 128\n",
        "activation = nn.ReLU()\n",
        "\n",
        "# Work in progress\n",
        "@use_named_args(dimensions=space)\n",
        "def fitness(learning_rate, regularization, conv1_channels, conv2_channels, linear1_neurons, linear2_neurons):\n",
        "    \"\"\"Perform Bayesian Hyperparameter tuning.\"\"\"\n",
        "\n",
        "    model = CNN(\n",
        "        conv1_channels=conv1_channels,\n",
        "        conv2_channels=conv2_channels,\n",
        "        linear1_neurons=linear1_neurons,\n",
        "        linear2_neurons=linear2_neurons,\n",
        "    )\n",
        "    model.to(device)\n",
        "    optimizer = optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=regularization)\n",
        "    # Create k-fold cross validation\n",
        "    avg_best_loss, *_ = kfold_cv_train(dataset=kfold_data, model=model, optimizer=optimizer, batch_size=batch_size)\n",
        "    # print(f'Avg. best validation loss: {sum(best_losses)/n_splits}')\n",
        "\n",
        "    return avg_best_loss\n",
        "\n",
        "\n",
        "n_calls = 100\n",
        "# Hyperparemeter search using Gaussian process minimization\n",
        "gp_result = gp_minimize(\n",
        "    func=fitness,\n",
        "    x0=default_params,\n",
        "    dimensions=space,\n",
        "    n_calls=n_calls,\n",
        "    random_state=SEED,\n",
        "    verbose=True,\n",
        "    callback=[tqdm_skopt(total=n_calls, desc=\"Gaussian Process\")],\n",
        ")\n",
        "\n",
        "plot_convergence(gp_result)\n",
        "plot_objective(gp_result)\n",
        "plot_evaluations(gp_result)\n",
        "gp_result.x"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\n",
            "\n",
            "Gaussian Process:   0%|          | 0/100 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A/usr/local/lib/python3.7/dist-packages/torch/nn/modules/lazy.py:178: UserWarning: Lazy modules are a new feature under heavy development so changes to the API or functionality can happen at any moment.\n",
            "  warnings.warn('Lazy modules are a new feature under heavy development '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Iteration No: 1 started. Evaluating function at provided point.\n",
            "FOLD 0\n",
            "--------------------------------\n",
            "Stopping after 105 epochs due to no improvement.\n",
            "FOLD 1\n",
            "--------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L5GY3uJQHTSj"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}